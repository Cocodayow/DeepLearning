{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cogs185 HW1 Report\n",
    "\n",
    "### Jiawen Wang\n",
    "\n",
    "### Question 2\n",
    "\n",
    "#### Question 2.1\n",
    "##### The mathematical form of the gradient of the loss function\n",
    "\n",
    "Loss function is:\n",
    "$$ L(\\textbf{w}) = \\frac{1}{2} \\left \\| \\textbf{w} \\right \\|^2 + C \\sum_i \\max(0, 1 - y_i \\langle \\textbf{x}_i, \\textbf{w} \\rangle) $$\n",
    "\n",
    "The gradient w.r.t $\\textbf{w}$ of the loss function:\n",
    "$$ L'(\\textbf{w}) = \\frac{d L(\\textbf{w})}{d \\textbf{w}} = \\textbf{w} + C \\sum_i \\begin{cases} -y_i \\textbf{x}_i & \\text{if } y_i \\langle \\textbf{x}_i, \\textbf{w} \\rangle \\leq 1 \\\\ 0 & \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "#### Question 2.2\n",
    "##### The optimal w∗ = arg minw L(w) as the minimizer\n",
    "\n",
    "The optimal w* value is\n",
    "[[ 0.15983746  6.39176826 -4.40728501]\\\n",
    " [ 0.2970873  -0.31933683 -0.92796169]\\\n",
    " [ 0.45801751 -2.25023377 -1.96619639]\\\n",
    " [-1.0526293   1.6890044   2.00050391]\\\n",
    " [-0.60438444 -3.30471002  3.70373197]]\n",
    "\n",
    "The C value associated is 10.0. This w is optimal because compared to all the other w coming from C values, {0.5, 2.0, 5.0},\\\n",
    "Since all the testing accuracies are the same- 100% for different C values, we will prioritize the one with greatest training accuracy.\\\n",
    "This w gives the greatest training accuracy of 95.83333333333334 %, makes our training model the most fittable.\n",
    "\n",
    "#### Question 2.3\n",
    "##### Training accuracy and test accuracy with C = 0.5, 2.0, 5.0, 10.0.\n",
    "\n",
    "C = 0.5\n",
    "Total training accuracy: 94.16666666666667 %.\\\n",
    "Total test accuracy: 100.0 %.\\\n",
    "C = 2.0\\\n",
    "Total training accuracy: 91.66666666666666 %.\\\n",
    "Total test accuracy: 100.0 %.\\\n",
    "C = 5.0\\\n",
    "Total training accuracy: 94.16666666666667 %.\\\n",
    "Total test accuracy: 100.0 %.\\\n",
    "C = 10.0\\\n",
    "Total training accuracy: 95.83333333333334 %.\\\n",
    "Total test accuracy: 100.0 %.\n",
    "\n",
    "#### Question 2.4\n",
    "##### Plot training data along with decision boundaries (w∗1, . . . , w∗K), K = 3, using the first two dimensions of the features for x.\n",
    "![alt text](report2_4.png)\n",
    "\n",
    "### Question 3\n",
    "\n",
    "#### Question 3.1\n",
    "##### The mathematical form of the gradient of the loss function.\n",
    "\n",
    "Loss function is:\n",
    "\n",
    "$ L(\\textbf{w}_1, \\dots, \\textbf{w}_K) = \\frac{1}{2} \\sum_{k=1}^{K}\\left \\| \\textbf{w}_k \\right \\|^2 \\nonumber + C \\sum_{i} \\sum_{k=1, k\\ne y_i}^K max\\big( 0, 1- (<\\textbf{w}_{y_i}, \\textbf{x}_i> - <\\textbf{w}_k, \\textbf{x}_i>) \\big) \\nonumber $\n",
    "\n",
    "The gradient w.r.t $\\textbf{w}$ of the loss function:\n",
    "\n",
    "$ L'(\\textbf{w}_b) = \\frac{dL(\\textbf{w}_b)}{d \\textbf{w}_b} = \\textbf{w}_b + C \\sum_{i} \\sum_{k=1, k\\ne y_i}^K\\left\\{\\begin{matrix}\n",
    " \\textbf{x}_i& \\texttt{, if } (b=k) \\wedge  (b\\neq y_i) \\wedge (<\\textbf{w}_{y_i}, \\textbf{x}_i>-<\\textbf{w}_k, \\textbf{x}_i> < 1)\\\\ \n",
    " -\\textbf{x}_i & \\texttt{, if } (b \\neq k) \\wedge  (b= y_i) \\wedge (<\\textbf{w}_{y_i}, \\textbf{x}_i>-<\\textbf{w}_k, \\textbf{x}_i> < 1) \\\\ \n",
    " 0 & \\texttt{, otherwise} \n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "#### Question 3.2\n",
    "##### The optimal (w∗1, . . . , w∗K) = arg minw1,...,wK L(w1, . . . , wK) as the minimizer.\n",
    "\n",
    "The optimal w value is\n",
    "[[ 0.24049589  0.61570382 -0.85619971]\\\n",
    " [ 0.54962307  0.34553133 -0.8951544 ]\\\n",
    " [ 0.94533576  0.151773   -1.09710876]\\\n",
    " [-1.40369287 -0.18671239  1.59040526]\\\n",
    " [-0.77774176 -0.88098094  1.6587227 ]]\n",
    "\n",
    "The C value associated is 2.0. This w is optimal because compared to all the other w coming from C values, {0.5, 5.0, 10.0},\\\n",
    "Since all the testing accuracies are the same - 100% for different C values, we will prioritize the one with greatest training accuracy.\\\n",
    "This w gives the greatest training accuracy of 97.5 %, fitting our training model more. Also, notice that when C = 10.0, the training accuracy\\\n",
    "is also 97.5%. However, since when C = 2.0, the Gradient descent converged fastest - after 2136 iterations. It is the most accurate and\\\n",
    "computationally efficient, therefore we this w is optimal.\n",
    "\n",
    "#### Question 3.3\n",
    "##### Training accuracy and test accuracy with C = 0.5, 2.0, 5.0, 10.0\n",
    "\n",
    "C = 0.5\n",
    "Total training accuracy: 96.66666666666667 %.\\\n",
    "Total testing accuracy: 100.0 %.\n",
    "\n",
    "C = 2.0\\\n",
    "Total training accuracy: 97.5 %.\\\n",
    "Total testing accuracy:  100.0 %.\n",
    "\n",
    "C = 5.0\\\n",
    "Total training accuracy: 95.83333333333334 %.\\\n",
    "Total testing accuracy: 100.0 %.\n",
    "\n",
    "C = 10.0\\\n",
    "Total training accuracy: 97.5 %.\\\n",
    "Total testing accuracy:  100.0 %.\n",
    "\n",
    "#### Question 3.4\n",
    "##### Plot training data along with decision boundaries (w∗1, . . . , w∗K), K = 3, using the first two dimensions of the features for x.\n",
    "![alt text](report3_4.png)\n",
    "\n",
    "### Question 4\n",
    "\n",
    "#### Question 4.1\n",
    "##### The mathematical form of the gradient of the loss function.\n",
    "\n",
    "Loss function is:\n",
    "\n",
    "$ L({\\bf w}_1, \\dots, {\\bf w}_K, b_1, \\dots, b_K) = -\\sum_{i} \\ln p_{y^{(i)}} + \\frac{\\lambda}{2} \\sum_{k=1}^{K}\\left \\| {\\bf w}_k \\right \\|^2. \\nonumber $\n",
    "\n",
    "where $ p_j = p(y=j|{\\bf x})=\\frac{e^{f_j}}{\\sum_{k=1}^{K}{e^{f_k}}}\\nonumber $; $ f_j =\\mathbf{w}_j \\cdot {\\bf x}+b_j $\n",
    "\n",
    "The gradient w.r.t $\\textbf{w}$ of the loss function:\n",
    "\n",
    "$ \\frac{d L({\\bf w}_1, \\dots, {\\bf w}_K, b_1, \\dots, b_K)}{d \\textbf{w}_k} = \\lambda \\textbf{w}_k + \\sum_{i,y_i=k}(p(k|{\\bf x}_i)-1){\\bf x}_i+\\sum_{i,y_i\\neq k}p(k|{\\bf x}_i){\\bf x}_i.$\n",
    "\n",
    "$ \\frac{d L({\\bf w}_1, \\dots, {\\bf w}_K, b_1, \\dots, b_K)}{d b_k} = \\sum_{i,y_i=k}(p(k|{\\bf x}_i)-1)+\\sum_{i,y_i\\neq k}p(k|{\\bf x}_i).$\n",
    "\n",
    "#### Question 4.2\n",
    "##### The optimal (w∗1, . . . , w∗K, b∗1, . . . , b∗K) = arg minw1,...,wK,b1,...,bK L(w1, . . . , wK, b1, . . . , bK) as the minimizer.\n",
    "\n",
    "The optimal w value is:\n",
    " [[ 0.32541817  0.66260006  1.66815353 -2.3413803  -1.0726503 ]\\\n",
    " [ 0.47169721  0.50819969 -0.32458591  0.02619788 -0.93840975]\\\n",
    " [-0.79711538 -1.17079975 -1.34356762  2.31518242  2.01106005]]\\\n",
    "The optimal b value is [[ 0.33784895] [ 0.48593311] [-0.82378206]]\n",
    "\n",
    "The C value associated is 0.1. This w is optimal because compared to all the other w coming from lambda values {0, $10^{-5}$, $10^{-3}$}, this w value is the smallest,\\\n",
    "and it results in the greatest training & testing accuracy, least number of iterations to convergence.\\\n",
    "Since all the training and testing accuracies are the same - 97.5% and 100% for different C values, we will prioritize the one with least number of iterations to convergence\\\n",
    "and the smallest w.\\\n",
    "When lambda = 0.1, the Gradient descent converged fastest - after 5315 iterations. It is the most computationally efficient. Also, compared to other lambdas,\\\n",
    "the w value associated with it is the smallest. Therefore, this w is optimal.\n",
    " \n",
    "\n",
    "#### Question 4.3\n",
    "#####  Training accuracy and test accuracy with λ = 0, $10^{-5}$, $10^{-3}$, 0.1.\n",
    "\n",
    "w = 0:\n",
    "The training accuracy: 97.5 %.\\\n",
    "The test accuracy:  100.0 %.\n",
    "\n",
    "w = $10^{-5}$\\\n",
    "The training accuracy: 97.5 %.\\\n",
    "The test accuracy:  100.0 %.\n",
    "\n",
    "w = $10^{-3}$\\\n",
    "The training accuracy: 97.5 %.\\\n",
    "The test accuracy:  100.0 %.\n",
    "\n",
    "w = 0.1\\\n",
    "The training accuracy: 97.5 %.\\\n",
    "The test accuracy:  100.0 %.\n",
    "\n",
    "#### Question 4.4\n",
    "##### Plot training data along with decision boundaries (w∗1, . . . , w∗K, b∗1, . . . , b∗K), K =3 using the first two dimensions of the features for x.\n",
    "![alt text](report4_4.png)\n",
    "\n",
    "\n",
    "### Question 5\n",
    "\n",
    "I used a random forest classifier, by constructing multiple decision trees. Random forest is robust and can be used for multi-class and binary classification.\\\n",
    "OvA on iris:\\\n",
    "The train accuracy1: 100.0 %.\\\n",
    "The train accuracy2: 100.0 %.\\\n",
    "The train accuracy3: 100.0 %.\\\n",
    "The train accuracy: 100.0 %.\\\n",
    "The test accuracy: 100.0 %.\n",
    "\n",
    "Explicit multiclass on iris:\\\n",
    "The train accuracy: 100.0 %.\\\n",
    "The test accuracy: 100.0 %.\n",
    "\n",
    "OvA on DNA:\\\n",
    "The train accuracy1: 100.0 %.\\\n",
    "The train accuracy2: 100.0 %.\\\n",
    "The train accuracy3: 100.0 %.\\\n",
    "The training accuracy: 100.0 %.\\\n",
    "The test accuracy: 93.00168634064082 %.\n",
    "\n",
    "Explicit multiclass on DNA:\\\n",
    "The training accuracy: 99.9 %.\\\n",
    "The test accuracy: 94.35075885328837 %.\n",
    "\n",
    "### Question 6\n",
    "##### Requirement: Write a brief report summarizing your work and compare the results. \n",
    "\n",
    "I implemented the one vs all SVMS using gradient descent. For each label in the dataset, a specific SVM classifier will be trained for the class.\\\n",
    "The classifier needs to compare one class to all the other classes. There are multiple binary classifiers for task1.\\\n",
    "Then, implemented explicit multi-class SVMs, which is a single classifier that handles multiple classes. \\\n",
    "Then, implemented a softmax classifier, using multiple hyperplanes to calculate probabilities.\\\n",
    "From my observation, softmax is the most computationally efficient, it takes a very short time for it to print the results.\\\n",
    "The other two algorithms are relatively slow. Between them, task2 model takes longer to run, which might be because task2 results in a more complex model.\\\n",
    "One of the reasons that decision boundaries look a bit off is because we have four dimensions feature space in the actual world, \\\n",
    "but we are only visualizing the first two dimensions. If we move to high dimensions, the boundaries will look more reasonable.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
