{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 Skeleton Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T08:13:57.455868Z",
     "start_time": "2018-02-19T08:13:57.434706Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import re\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import scipy.io as sio\n",
    "plt.rcParams['figure.figsize'] = 10,10\n",
    "\n",
    "import sklearn.datasets\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_iris_data = sklearn.datasets.load_iris()\n",
    "print (\"my_iris_data.data.shape:\",my_iris_data.data.shape)\n",
    "print (\"labels:\",my_iris_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T08:13:58.418914Z",
     "start_time": "2018-02-19T08:13:58.410253Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.concatenate((my_iris_data.data[10:50,:],my_iris_data.data[60:100,:], my_iris_data.data[110:150,:]))\n",
    "X_train = np.concatenate((np.ones((X_train.shape[0],1)),X_train),axis=1) # Append bias term 1\n",
    "y_train = np.concatenate((my_iris_data.target[10:50],my_iris_data.target[60:100], my_iris_data.target[110:150]))\n",
    "print (\"X_train.shape:\", X_train.shape)\n",
    "print (\"y_train.shape:\", y_train.shape)\n",
    "\n",
    "X_test = np.concatenate((my_iris_data.data[40:50,:],my_iris_data.data[90:100,:], my_iris_data.data[140:150,:]))\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0],1)),X_test),axis=1) # Append bias term 1\n",
    "y_test = np.concatenate((my_iris_data.target[40:50],my_iris_data.target[90:100], my_iris_data.target[140:150]))\n",
    "print (\"X_test.shape:\", X_test.shape)\n",
    "print (\"y_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the labels to get 3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train1 = np.copy(y_train); y_test1 = np.copy(y_test)\n",
    "y_train2 = np.copy(y_train); y_test2 = np.copy(y_test)\n",
    "y_train3 = np.copy(y_train); y_test3 = np.copy(y_test)\n",
    "\n",
    "y_train1[y_train == 1] = -1\n",
    "y_train1[y_train == 2] = -1\n",
    "y_train1[y_train == 0] = 1\n",
    "y_test1[y_test == 1] = -1\n",
    "y_test1[y_test == 2] = -1\n",
    "y_test1[y_test == 0] = 1\n",
    "\n",
    "y_train2[y_train == 1] = 1\n",
    "y_train2[y_train == 2] = -1\n",
    "y_train2[y_train == 0] = -1\n",
    "y_test2[y_test == 1] = 1\n",
    "y_test2[y_test == 2] = -1\n",
    "y_test2[y_test == 0] = -1\n",
    "\n",
    "y_train3[y_train == 1] = -1\n",
    "y_train3[y_train == 2] = 1\n",
    "y_train3[y_train == 0] = -1\n",
    "y_test3[y_test == 1] = -1\n",
    "y_test3[y_test == 2] = 1\n",
    "y_test3[y_test == 0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3: Softmax on iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original target function: \n",
    "\n",
    "$ L({\\bf w}_1, \\dots, {\\bf w}_K, b_1, \\dots, b_K) = -\\sum_{i} \\ln p_{y^{(i)}} + \\frac{\\lambda}{2} \\sum_{k=1}^{K}\\left \\| {\\bf w}_k \\right \\|^2. \\nonumber $\n",
    "\n",
    "where $ p_j = p(y=j|{\\bf x})=\\frac{e^{f_j}}{\\sum_{k=1}^{K}{e^{f_k}}}\\nonumber $; $ f_j =\\mathbf{w}_j \\cdot {\\bf x}+b_j $\n",
    "The gradient w.r.t $\\textbf{w}$ of the target function:\n",
    "\n",
    "$ \\frac{d L({\\bf w}_1, \\dots, {\\bf w}_K, b_1, \\dots, b_K)}{d \\textbf{w}_k} = \\lambda \\textbf{w}_k + \\sum_{i,y_i=k}(p(k|{\\bf x}_i)-1){\\bf x}_i+\\sum_{i,y_i\\neq k}p(k|{\\bf x}_i){\\bf x}_i.$\n",
    "\n",
    "$ \\frac{d L({\\bf w}_1, \\dots, {\\bf w}_K, b_1, \\dots, b_K)}{d b_k} = \\sum_{i,y_i=k}(p(k|{\\bf x}_i)-1)+\\sum_{i,y_i\\neq k}p(k|{\\bf x}_i).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb=0.001 # Set the lambda for task3\n",
    "learning_rate = 0.0001 # the alpha\n",
    "n_iter = 20000\n",
    "iterations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_P(X, W, b):\n",
    "    f         = W.dot(X.T) + b   # Shape: [K,n].\n",
    "    f         = f - f.max(axis=0, keepdims=True)  # Avoid the big number \n",
    "    exp_f     = np.exp(f)        # Shape: [K,n].\n",
    "    sum_exp_f = exp_f.sum(axis=0, keepdims=True)  # Shape: [1,n].\n",
    "    P         = (exp_f / sum_exp_f).T\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient of loss function L(w)\n",
    "def L_prime(X, Y, W, b):\n",
    "    ########### YOUR CODE HERE ###########\n",
    "\n",
    "\n",
    "    return dL_by_dW, dL_by_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(X, Y, W, b):\n",
    "    ########### YOUR CODE HERE ###########\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Warning! The next cell takes time to finish descending!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "\n",
    "# NOTE: Shape of w matrix is different from Task 2!\n",
    "w = np.zeros((K, X_train.shape[1]))\n",
    "b = np.zeros((K,1))   # Bias vector.\n",
    "\n",
    "# We will keep track of training loss over iterations\n",
    "iterations = [0]\n",
    "L_list = [L(X_train, y_train, w, b)]\n",
    "for i in range(n_iter):\n",
    "    gradient_w, gradient_b = L_prime(X_train, y_train, w, b)\n",
    "    w_new = ########### YOUR CODE HERE ###########\n",
    "    b_new = ########### YOUR CODE HERE ###########\n",
    "    iterations.append(i+1)\n",
    "    L_list.append(L(X_train, y_train, w_new, b_new))\n",
    "\n",
    "    if (np.linalg.norm(w_new - w, ord = 1) + np.linalg.norm(b_new - b, ord = 1)) < 0.0005:\n",
    "        print(\"gradient descent has converged after \" + str(i) + \" iterations\")\n",
    "        break\n",
    "    if i % 1000 == 0:\n",
    "        print(i, np.linalg.norm(w_new - w, ord = 1) + np.linalg.norm(b_new - b, ord = 1), L_list[-1])\n",
    "    w = w_new\n",
    "    b = b_new\n",
    "print (\"w vector: \\n\" + str(w))\n",
    "print (\"b vector: \\n\" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(np.dot(X_train, w.T) + b.T,axis=1)\n",
    "training_accuracy = (prediction - y_train == 0)\n",
    "\n",
    "print \"The training accuracy:\", np.sum(training_accuracy)*1.0/X_train.shape[0]*100, \"%.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 1; x2 = 2\n",
    "\n",
    "x = np.arange(np.min(X_train[:,x1])-1,np.max(X_train[:,x1])+1,1.0)\n",
    "y1 = (-b[0][0]-w[0][1]*x)/w[0][2]\n",
    "y2 = (-b[1][0]-w[1][1]*x)/w[1][2]\n",
    "y3 = (-b[2][0]-w[2][1]*x)/w[2][2]\n",
    "\n",
    "plt.scatter(X_train[y_train==0, x1], X_train[y_train==0, x2], marker='x', color='r', alpha=0.7, s=10, label='class 0')\n",
    "plt.scatter(X_train[y_train==1, x1], X_train[y_train==1, x2], marker='o', color='g', alpha=0.7, s=10, label='class 1')\n",
    "plt.scatter(X_train[y_train==2, x1], X_train[y_train==2, x2], marker='o', color='b', alpha=0.7, s=10, label='class 2')\n",
    "\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.plot(x,y1, color='r', label='decision boundary 1')\n",
    "plt.plot(x,y2, color='g', label='decision boundary 2')\n",
    "plt.plot(x,y3, color='b', label='decision boundary 3')\n",
    "plt.title('Training data and decision boundary')\n",
    "\n",
    "plt.legend(loc='upper right', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(np.dot(X_test, w.T) + b.T, axis=1)\n",
    "\n",
    "testing_accuracy = np.sum(prediction == y_test)*1.0/X_test.shape[0]\n",
    "print (\"The test accuracy: \", testing_accuracy*100, \"%.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
