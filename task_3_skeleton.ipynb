{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 Skeleton Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T08:13:57.455868Z",
     "start_time": "2018-02-19T08:13:57.434706Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import re\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import scipy.io as sio\n",
    "plt.rcParams['figure.figsize'] = 10,10\n",
    "\n",
    "import sklearn.datasets\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_iris_data.data.shape: (150, 4)\n",
      "labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "my_iris_data = sklearn.datasets.load_iris()\n",
    "print (\"my_iris_data.data.shape:\",my_iris_data.data.shape)\n",
    "print (\"labels:\",my_iris_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T08:13:58.418914Z",
     "start_time": "2018-02-19T08:13:58.410253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (120, 5)\n",
      "y_train.shape: (120,)\n",
      "X_test.shape: (30, 5)\n",
      "y_test.shape: (30,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate((my_iris_data.data[10:50,:],my_iris_data.data[60:100,:], my_iris_data.data[110:150,:]))\n",
    "X_train = np.concatenate((np.ones((X_train.shape[0],1)),X_train),axis=1) # Append bias term 1\n",
    "y_train = np.concatenate((my_iris_data.target[10:50],my_iris_data.target[60:100], my_iris_data.target[110:150]))\n",
    "print (\"X_train.shape:\", X_train.shape)\n",
    "print (\"y_train.shape:\", y_train.shape)\n",
    "\n",
    "X_test = np.concatenate((my_iris_data.data[40:50,:],my_iris_data.data[90:100,:], my_iris_data.data[140:150,:]))\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0],1)),X_test),axis=1) # Append bias term 1\n",
    "y_test = np.concatenate((my_iris_data.target[40:50],my_iris_data.target[90:100], my_iris_data.target[140:150]))\n",
    "print (\"X_test.shape:\", X_test.shape)\n",
    "print (\"y_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the labels to get 3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train1 = np.copy(y_train); y_test1 = np.copy(y_test)\n",
    "y_train2 = np.copy(y_train); y_test2 = np.copy(y_test)\n",
    "y_train3 = np.copy(y_train); y_test3 = np.copy(y_test)\n",
    "\n",
    "y_train1[y_train == 1] = -1\n",
    "y_train1[y_train == 2] = -1\n",
    "y_train1[y_train == 0] = 1\n",
    "y_test1[y_test == 1] = -1\n",
    "y_test1[y_test == 2] = -1\n",
    "y_test1[y_test == 0] = 1\n",
    "\n",
    "y_train2[y_train == 1] = 1\n",
    "y_train2[y_train == 2] = -1\n",
    "y_train2[y_train == 0] = -1\n",
    "y_test2[y_test == 1] = 1\n",
    "y_test2[y_test == 2] = -1\n",
    "y_test2[y_test == 0] = -1\n",
    "\n",
    "y_train3[y_train == 1] = -1\n",
    "y_train3[y_train == 2] = 1\n",
    "y_train3[y_train == 0] = -1\n",
    "y_test3[y_test == 1] = -1\n",
    "y_test3[y_test == 2] = 1\n",
    "y_test3[y_test == 0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3: Softmax on iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original target function: \n",
    "\n",
    "$ L({\\bf w}_1, \\dots, {\\bf w}_K, b_1, \\dots, b_K) = -\\sum_{i} \\ln p_{y^{(i)}} + \\frac{\\lambda}{2} \\sum_{k=1}^{K}\\left \\| {\\bf w}_k \\right \\|^2. \\nonumber $\n",
    "\n",
    "where $ p_j = p(y=j|{\\bf x})=\\frac{e^{f_j}}{\\sum_{k=1}^{K}{e^{f_k}}}\\nonumber $; $ f_j =\\mathbf{w}_j \\cdot {\\bf x}+b_j $\n",
    "\n",
    "The gradient w.r.t $\\textbf{w}$ of the target function:\n",
    "\n",
    "$ \\frac{d L({\\bf w}_1, \\dots, {\\bf w}_K, b_1, \\dots, b_K)}{d \\textbf{w}_k} = \\lambda \\textbf{w}_k + \\sum_{i,y_i=k}(p(k|{\\bf x}_i)-1){\\bf x}_i+\\sum_{i,y_i\\neq k}p(k|{\\bf x}_i){\\bf x}_i.$\n",
    "\n",
    "$ \\frac{d L({\\bf w}_1, \\dots, {\\bf w}_K, b_1, \\dots, b_K)}{d b_k} = \\sum_{i,y_i=k}(p(k|{\\bf x}_i)-1)+\\sum_{i,y_i\\neq k}p(k|{\\bf x}_i).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb=0.001 # Set the lambda for task3\n",
    "learning_rate = 0.0001 # the alpha\n",
    "n_iter = 20000\n",
    "iterations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_P(X, W, b):\n",
    "    f         = W.dot(X.T) + b   # Shape: [K,n].\n",
    "    f         = f - f.max(axis=0, keepdims=True)  # Avoid the big number \n",
    "    exp_f     = np.exp(f)        # Shape: [K,n].\n",
    "    sum_exp_f = exp_f.sum(axis=0, keepdims=True)  # Shape: [1,n].\n",
    "    P         = (exp_f / sum_exp_f).T\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient of loss function L(w)\n",
    "def L_prime(X, Y, W, b):\n",
    "    ########### YOUR CODE HERE ###########\n",
    "    N = X.shape[0]\n",
    "    Y_hat = sigmoid(np.dot(X, W) + b.T) \n",
    "    error = Y_hat - Y\n",
    "    \n",
    "    dL_by_dW = np.dot(X.T, error) / N\n",
    "    dL_by_db = np.sum(error, axis=0, keepdims=True) / N  \n",
    "    return dL_by_dW, dL_by_db.T  \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(X, Y, W, b):\n",
    "    ########### YOUR CODE HERE ###########\n",
    "    Z = np.dot(X, W) + b.T  # Broadcasting addition of bias across all samples\n",
    "    Y_hat = sigmoid(Z)  # Apply sigmoid to get predictions\n",
    "    loss = -np.mean(Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat))  # Compute cross-entropy loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Warning! The next cell takes time to finish descending!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (120,5) and (3,5) not aligned: 5 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# We will keep track of training loss over iterations\u001b[39;00m\n\u001b[0;32m      8\u001b[0m iterations \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m L_list \u001b[38;5;241m=\u001b[39m [\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[0;32m     11\u001b[0m     gradient_w, gradient_b \u001b[38;5;241m=\u001b[39m L_prime(X_train, y_train, w, b)\n",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m, in \u001b[0;36mL\u001b[1;34m(X, Y, W, b)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mL\u001b[39m(X, Y, W, b):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m########### YOUR CODE HERE ###########\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     Z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;66;03m# Broadcasting addition of bias across all samples\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     Y_hat \u001b[38;5;241m=\u001b[39m sigmoid(Z)  \u001b[38;5;66;03m# Apply sigmoid to get predictions\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(Y \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(Y_hat) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m Y) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m Y_hat))  \u001b[38;5;66;03m# Compute cross-entropy loss\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (120,5) and (3,5) not aligned: 5 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "K = 3\n",
    "\n",
    "# NOTE: Shape of w matrix is different from Task 2!\n",
    "w = np.zeros((K, X_train.shape[1]))\n",
    "b = np.zeros((K,1))   # Bias vector.\n",
    "\n",
    "# We will keep track of training loss over iterations\n",
    "iterations = [0]\n",
    "L_list = [L(X_train, y_train, w, b)]\n",
    "for i in range(n_iter):\n",
    "    gradient_w, gradient_b = L_prime(X_train, y_train, w, b)\n",
    "    w_new = w - learning_rate * gradient_w\n",
    "    b_new = b - learning_rate * gradient_b\n",
    "    iterations.append(i+1)\n",
    "    L_list.append(L(X_train, y_train, w_new, b_new))\n",
    "\n",
    "    if (np.linalg.norm(w_new - w, ord = 1) + np.linalg.norm(b_new - b, ord = 1)) < 0.0005:\n",
    "        print(\"gradient descent has converged after \" + str(i) + \" iterations\")\n",
    "        break\n",
    "    if i % 1000 == 0:\n",
    "        print(i, np.linalg.norm(w_new - w, ord = 1) + np.linalg.norm(b_new - b, ord = 1), L_list[-1])\n",
    "    w = w_new\n",
    "    b = b_new\n",
    "print (\"w vector: \\n\" + str(w))\n",
    "print (\"b vector: \\n\" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(np.dot(X_train, w.T) + b.T,axis=1)\n",
    "training_accuracy = (prediction - y_train == 0)\n",
    "\n",
    "print \"The training accuracy:\", np.sum(training_accuracy)*1.0/X_train.shape[0]*100, \"%.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 1; x2 = 2\n",
    "\n",
    "x = np.arange(np.min(X_train[:,x1])-1,np.max(X_train[:,x1])+1,1.0)\n",
    "y1 = (-b[0][0]-w[0][1]*x)/w[0][2]\n",
    "y2 = (-b[1][0]-w[1][1]*x)/w[1][2]\n",
    "y3 = (-b[2][0]-w[2][1]*x)/w[2][2]\n",
    "\n",
    "plt.scatter(X_train[y_train==0, x1], X_train[y_train==0, x2], marker='x', color='r', alpha=0.7, s=10, label='class 0')\n",
    "plt.scatter(X_train[y_train==1, x1], X_train[y_train==1, x2], marker='o', color='g', alpha=0.7, s=10, label='class 1')\n",
    "plt.scatter(X_train[y_train==2, x1], X_train[y_train==2, x2], marker='o', color='b', alpha=0.7, s=10, label='class 2')\n",
    "\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.plot(x,y1, color='r', label='decision boundary 1')\n",
    "plt.plot(x,y2, color='g', label='decision boundary 2')\n",
    "plt.plot(x,y3, color='b', label='decision boundary 3')\n",
    "plt.title('Training data and decision boundary')\n",
    "\n",
    "plt.legend(loc='upper right', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(np.dot(X_test, w.T) + b.T, axis=1)\n",
    "\n",
    "testing_accuracy = np.sum(prediction == y_test)*1.0/X_test.shape[0]\n",
    "print (\"The test accuracy: \", testing_accuracy*100, \"%.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
